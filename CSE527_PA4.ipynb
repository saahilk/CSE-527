{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "CSE527_PA4_fall_21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "561f70b3cf33455294cb6aebe4da1803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_863d45324c4741ab8ce0e60c9e955ee9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6fc6eaa216d14784af1d497ecc49491b",
              "IPY_MODEL_65d5e48555644b3b8ace017b9f67e859",
              "IPY_MODEL_d707cb773a5e4e73a25cafc8ee9192f7"
            ]
          }
        },
        "863d45324c4741ab8ce0e60c9e955ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fc6eaa216d14784af1d497ecc49491b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_833c5391a8d449da956a156a187bdf56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e73ade76820f4a5882d9e898e9b2db7e"
          }
        },
        "65d5e48555644b3b8ace017b9f67e859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_179be53aae0943de8e95fdf524eb6c6f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 133546016,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 133546016,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6eefd20fb36747bdb3acf6c4a7dcb06d"
          }
        },
        "d707cb773a5e4e73a25cafc8ee9192f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c38733ec72a4a6dacf9c62db0fa0fa7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 127M/127M [00:02&lt;00:00, 52.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cddb8a7431a948eabf1db8a61c21ebc8"
          }
        },
        "833c5391a8d449da956a156a187bdf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e73ade76820f4a5882d9e898e9b2db7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "179be53aae0943de8e95fdf524eb6c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6eefd20fb36747bdb3acf6c4a7dcb06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c38733ec72a4a6dacf9c62db0fa0fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cddb8a7431a948eabf1db8a61c21ebc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f74d922aa5ab45638164cdacde73ec0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a3bb4e35c1b4a048cb4bf76ad430351",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e02ed5a83514724984416732700ef11",
              "IPY_MODEL_8c76e72cdd5843e4a613219b25469a0c",
              "IPY_MODEL_b57b42aada7246eb891a7b487a5b22d6"
            ]
          }
        },
        "1a3bb4e35c1b4a048cb4bf76ad430351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e02ed5a83514724984416732700ef11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e55537965fc45d19905164e38d47075",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ffb5dd056ce46d8993be60c3e2876a4"
          }
        },
        "8c76e72cdd5843e4a613219b25469a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce6ebce546334ed89a1069b13f9ce12d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244408911,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244408911,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4564d782bd0f40138739e408029e2120"
          }
        },
        "b57b42aada7246eb891a7b487a5b22d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b3f261be53a4cba84e7a4a67aae316f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:05&lt;00:00, 41.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ccd276c75ea4f91a3d9d930c8bdb8ab"
          }
        },
        "6e55537965fc45d19905164e38d47075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ffb5dd056ce46d8993be60c3e2876a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce6ebce546334ed89a1069b13f9ce12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4564d782bd0f40138739e408029e2120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b3f261be53a4cba84e7a4a67aae316f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ccd276c75ea4f91a3d9d930c8bdb8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSfnuObtYYMH"
      },
      "source": [
        "# CSE527 Programming Assignment 4\n",
        "**Due date: 23:59 on November 26th 2021**\n",
        "\n",
        "In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n",
        "\n",
        "## Google Colab Tutorial\n",
        "---\n",
        "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
        "\n",
        "Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
        "\n",
        "\n",
        "## Description\n",
        "---\n",
        "You train a deep network from scratch if you have enough data (it's not always obvious whether or not you do), and if you cannot then instead you fine-tune a pre-trained network as in this problem.\n",
        "\n",
        "In Problem 1, you will be finetuning a pretrained resnet and using it to classify JPL interaction video frames. \n",
        "\n",
        "For Problem 2, you are going to use thread pooling/convolution to classify the video files using the similar pretrained network as your baseline model.\n",
        "\n",
        "\n",
        "\n",
        "There are 2 problems in this homework with a total of 120 points including 20 bonus points. Be sure to read **Submission Guidelines** below. They are important. For the problems requiring text descriptions, you might want to add a markdown block for that.\n",
        "\n",
        "## Dataset\n",
        "---\n",
        "Save the dataset(click me) into your working folder in your Google Drive for this homework. <br>\n",
        "Under your root folder, there should be a folder named \"data\" (i.e. XXX/Surname_Givenname_SBUID/data) containing the images.\n",
        "**Do not upload** the data subfolder before submitting on blackboard due to size limit. There should be only one .ipynb file under your root folder Surname_Givenname_SBUID.\n",
        "\n",
        "## Some Tutorials (PyTorch)\n",
        "---\n",
        "- You will be using PyTorch for deep learning toolbox (follow the [link](http://pytorch.org) for installation).\n",
        "- For PyTorch beginners, please read this [tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) before doing your homework.\n",
        "- Feel free to study more tutorials at http://pytorch.org/tutorials/.\n",
        "- Find cool visualization here at http://playground.tensorflow.org.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UA6WFgcYYMI"
      },
      "source": [
        "# import packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random \n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7T72O-1ks-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f59045-a0bb-402e-8b7f-3e3571673ef9"
      },
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive._mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIT3WIuykrg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d09146-831c-488f-8c55-0151aa0260d5"
      },
      "source": [
        "# Set your working directory (in your google drive)\n",
        "#   change it to your specific homework directory.\n",
        "%cd '/content/gdrive/MyDrive/Kamat_SaahilSuhas_114360951_PA4'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Kamat_SaahilSuhas_114360951_PA4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nBHKIxzAYYM2"
      },
      "source": [
        "## Problem 1 Data Preparation and Fine-tuning\n",
        "## First-Person Activity Recognition: What Are They Doing to Me?\n",
        "In this part of the assignment, you will implement an Activity Classifier using JPL dataset. You will use an an ImageNet pre-trained CNN that serves as a feature extractor.\n",
        "## About JPL dataset\n",
        "This first-person dataset contains videos of interactions between humans and the observer. We attached a GoPro2 camera to the head of our humanoid model, and asked human participants to interact with the humanoid by performing activities. In order to emulate the mobility of a real robot, we also placed wheels below the humanoid and made an operator to move the humanoid by pushing it from the behind. Videos were recorded continuously during human activities where each video sequence contains 0 to 3 activities. The videos are in 320*240 resolution with 30 fps.\n",
        "\n",
        "There are 7 different types of activities in the dataset, including :\n",
        "<ol>\n",
        "\n",
        "###4 positive (i.e., friendly) interactions with the observer: \n",
        "\n",
        "<li> 'Shaking hands with the observer', <li> 'hugging the observer', <li> 'petting the observer', and <li> 'waving a hand to the observer' \n",
        "\n",
        "###1 neutral interaction:\n",
        "<li>  the situation where two persons have a conversation about the observer while occasionally pointing it.\n",
        "\n",
        "###2 negative (i.e., hostile) interactions: \n",
        "<li>  'Punching the observer' and <li> 'throwing objects to the observer'\n",
        "</ol>\n",
        "We will thus assign label to each action, for example:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  'Shaking hands with the observer': 1, \n",
        "  'hugging the observer': 2, \n",
        "  'petting the observer': 3, \n",
        "  'waving a hand to the observer': 4,\n",
        "  'the situation where two persons have a conversation about the observer while occasionally pointing it': 5,\n",
        "  'Punching the observer': 6,\n",
        "  'throwing objects to the observer': 7\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ulgWN7XKFH"
      },
      "source": [
        "### Problem 1.0\n",
        "## Loading the JPL dataset: 5 points\n",
        "Check the segmented version from [here](https://drive.google.com/file/d/1eivyF3gPbS3ejea-NYebMBzS40xsRrqF/view?usp=sharing). \n",
        "Save the videos into your working folder in your Google Drive.\n",
        "Under your root folder, there should be a folder named \"data\" (i.e. XXX/Surname_Givenname_SBUID/data) containing the jpl_vid directory where you should extract the jpl dataset. Do not upload the data subfolder before submitting on blackboard due to size limit. There should be only one .ipynb file under your root folder Surname_Givenname_SBUID. \n",
        "In the first part of data preparation, we will convert the videos into images. We will only use all frames from each video and store them as .jpg files. The data folder now also consists of two other directories: jpl_vid, jpl_img. **We will delete the jpl_img directory from you data folder before evaluating.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhXSS0jmpyxf"
      },
      "source": [
        "# !rmdir ./data/jpl_img/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVtNIWCRXBvT"
      },
      "source": [
        "def get_frames(filename, n_frames= -1):\n",
        "#--------------------------------------------------\n",
        "#       given the filename of a video,  generate all the frames for that video and return it with length of the frame\n",
        "#       Example: if path /data/jpl_img/10_1/ should contain the frames from 10_1.avi\n",
        "\n",
        "#       if n_frames is -1 store all the frames of the video. \n",
        "#       Else we will only use n_frames frames from each video that are equally spaced across the entire video and store them as .jpg files.\n",
        "#       We expect you to use CV2 library to read video frames.\n",
        "#--------------------------------------------------\n",
        "    vidcap = cv2.VideoCapture(filename)\n",
        "    total_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    # print(total_frames)\n",
        "    if(n_frames==-1):\n",
        "      n_frames = total_frames\n",
        "    frames_step = total_frames//n_frames\n",
        "    imgs = []\n",
        "    for i in range(n_frames):\n",
        "      vidcap.set(1,i*frames_step)\n",
        "      success,image = vidcap.read()\n",
        "      imgs.append(image)\n",
        "    \n",
        "    frames = imgs\n",
        "    v_len = len(frames)\n",
        "    return frames, v_len\n",
        "    \n",
        "def store_frames(frames, path2store):\n",
        "    for ii, frame in enumerate(frames):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  \n",
        "        path2img = os.path.join(path2store, \"frame\"+str(ii)+\".jpg\")\n",
        "        cv2.imwrite(path2img, frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNC6g66hqfsY"
      },
      "source": [
        "import os\n",
        "\n",
        "path2data = \"./data\"\n",
        "sub_folder = \"jpl_vid\"\n",
        "sub_folder_jpg = \"jpl_img\"\n",
        "path2video = os.path.join(path2data, sub_folder)\n",
        "listOfCategories = os.listdir(path2video)\n",
        "# listOfCategories, len(listOfCategories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Aaig7TAu69D"
      },
      "source": [
        "n_frames = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyMtynYTqbu1"
      },
      "source": [
        "extension = \".avi\"\n",
        "#--------------------------------------------------\n",
        "#choose a value for n_frames below to optimize your solution. We might randomly choose n_frames while evaluating \n",
        "#--------------------------------------------------\n",
        "n_frames = 16\n",
        "for root, dirs, files in os.walk(path2video, topdown=False):\n",
        "    for name in files:\n",
        "        if extension not in name:\n",
        "            continue\n",
        "        path2vid = os.path.join(root, name)\n",
        "        frames, vlen = get_frames(path2vid, n_frames= n_frames)\n",
        "        path2store = path2vid.replace(sub_folder, sub_folder_jpg)\n",
        "        path2store = path2store.replace(extension, \"\")\n",
        "        print(path2store)\n",
        "        os.makedirs(path2store, exist_ok= True)\n",
        "        store_frames(frames, path2store)\n",
        "    print(\"-\"*50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cOZcputzDa0"
      },
      "source": [
        "## Training, Test and Validation set\n",
        "**Training set:**\n",
        "Participant 1-9\n",
        "\n",
        "\n",
        "**Test set:**\n",
        "Participant 10 - 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmntRcKHH_AO"
      },
      "source": [
        "def prepare_sets(path2ajpgs):\n",
        "    listOfCats = os.listdir(path2ajpgs)\n",
        "    train_id = []\n",
        "    train_label = []\n",
        "    test_id = []\n",
        "    test_label = []\n",
        "    for i in range(len(listOfCats)):\n",
        "      x = listOfCats[i].split(\"_\")\n",
        "      if(int(x[0]) > 9):\n",
        "        test_id.append(listOfCats[i])\n",
        "        test_label.append(int(x[1]))\n",
        "      else:\n",
        "        train_id.append(listOfCats[i])\n",
        "        train_label.append(int(x[1]))\n",
        "      \n",
        "    return train_id, train_label, test_id, test_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpqKf7A4eG0G"
      },
      "source": [
        "path2jpg = os.path.join(path2data, sub_folder_jpg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXyuLJBlIMKM"
      },
      "source": [
        "train_ids, train_labels, test_ids, test_labels = prepare_sets(path2jpg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu1dseEo2Ce8"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)\n",
        "torch.manual_seed(2020)\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, ids, labels,transform):      \n",
        "        self.transform = transform\n",
        "        self.ids = ids\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        path2imgs=glob.glob(path2jpg+\"/\"+self.ids[idx]+\"/*.jpg\")\n",
        "        path2imgs = path2imgs[:n_frames]\n",
        "        label = self.labels[idx]\n",
        "        frames = []\n",
        "        for p2i in path2imgs:\n",
        "            frame = Image.open(p2i)\n",
        "            frames.append(frame)\n",
        "        \n",
        "        seed = np.random.randint(1e9)        \n",
        "        frames_tr = []\n",
        "        for frame in frames:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            frame = self.transform(frame)\n",
        "            frames_tr.append(frame)\n",
        "        if len(frames_tr)>0:\n",
        "            frames_tr = torch.stack(frames_tr)\n",
        "        return frames_tr.to(device), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SviYj6QU05YL"
      },
      "source": [
        "### Problem 1.1\n",
        "## Dataloader: 5 points\n",
        "We now need to create scripts so that it accepts the generator that we just created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeetLEZe5XfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ecbdd9-dbcf-476f-b955-3959907d571e"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "train_ds = VideoDataset(ids= train_ids, labels= train_labels,transform=transforms.ToTensor())\n",
        "print(len(train_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeSo9wsuVJQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0c5dec-db9e-4b31-93cb-958278bdeeeb"
      },
      "source": [
        "test_ds = VideoDataset(ids= test_ids, labels= test_labels, transform= transforms.ToTensor())\n",
        "print(len(test_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj0D75HAVu8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bc7955-b21c-408d-cc82-84e100048ec2"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#create dataloader for all the datasets(train and test) \n",
        "#--------------------------------------------------\n",
        "\n",
        "batch_size = 8\n",
        "train_dl = DataLoader(train_ds,batch_size=batch_size)\n",
        "test_dl = DataLoader(test_ds,batch_size=batch_size)\n",
        "dataloaders = {'train':train_dl,'test':test_dl}\n",
        "data_siz = {'train':len(train_ds),'test':len(test_ds)}\n",
        "print(data_siz)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 63, 'test': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-fbkXXmWNdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53f8d18-85e0-43b5-ea9d-3a78b07d7740"
      },
      "source": [
        "for xb,yb in train_dl:\n",
        "    print(xb.shape, yb.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16, 3, 240, 320]) torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vRVJRwhKZe"
      },
      "source": [
        "## Problem 1.2: Fine Tuning a Pre-Trained Deep Network: 40 points\n",
        "The representations learned by deep convolutional networks generalize surprisingly well to other recognition tasks. \n",
        "\n",
        "But how do we use an existing deep network for a new recognition task? Take for instance,  [ResNet network](https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html) [(paper)](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "\n",
        "**Hints**:\n",
        "- Many pre-trained models are available in PyTorch at [here](http://pytorch.org/docs/master/torchvision/models.html).\n",
        "- For fine-tuning pretrained network using PyTorch, please read this [tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKHsertZhq_r"
      },
      "source": [
        "###Problem 1.2.1\n",
        "\n",
        "##*Fine-tune* an existing network: 30 points\n",
        " In this scenario you take an existing network, replace the final layer (or more) with random weights, and train the entire network again with images and ground truth labels for your recognition task. You are effectively treating the pre-trained deep network as a better initialization than the random weights used when training from scratch. When you don't have enough training data to train a complex network from scratch (e.g. with the 7 classes) this is an attractive option. In [this paper](http://www.cc.gatech.edu/~hays/papers/deep_geo.pdf) from CVPR 2015, there wasn't enough data to train a deep network from scratch, but fine tuning led to 4 times higher accuracy than using off-the-shelf networks directly.\n",
        " You are required to implement above strategy to fine-tune a pre-trained **ResNet** for this video frames classification task with 7 classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0hcgPqfIbzT"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = torch.swapaxes(inputs,1,2)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels-1\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # print(preds)\n",
        "                    # print(labels)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / data_siz[phase]\n",
        "            epoch_acc = running_corrects.double() / data_siz[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fiE7wsMlVu4"
      },
      "source": [
        "###Problem 1.2.2\n",
        "###Training and Testing your fine-tuned Network: 10 points\n",
        "You will fine-tune your network using every frame in the video as a sample with the class label. Use train_dl and test_dl and feed it to your fine-tuned network. Please provide detailed descriptions of:<br>\n",
        "(1) which layers of Resnet have been replaced<br>\n",
        "(2) the architecture of the new layers added including activation methods <br>\n",
        "(3) the final accuracy on test set <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAxTMdCXhmGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "561f70b3cf33455294cb6aebe4da1803",
            "863d45324c4741ab8ce0e60c9e955ee9",
            "6fc6eaa216d14784af1d497ecc49491b",
            "65d5e48555644b3b8ace017b9f67e859",
            "d707cb773a5e4e73a25cafc8ee9192f7",
            "833c5391a8d449da956a156a187bdf56",
            "e73ade76820f4a5882d9e898e9b2db7e",
            "179be53aae0943de8e95fdf524eb6c6f",
            "6eefd20fb36747bdb3acf6c4a7dcb06d",
            "4c38733ec72a4a6dacf9c62db0fa0fa7",
            "cddb8a7431a948eabf1db8a61c21ebc8"
          ]
        },
        "outputId": "2e4e92af-c034-49a2-c04a-089a0c008d59"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Fine-Tune Pretrained Network\n",
        "#--------------------------------------------------\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "num_classes = 7\n",
        "\n",
        "model_ft = models.video.r3d_18(pretrained=True)\n",
        "model_ft.to(device)\n",
        "set_parameter_requires_grad(model_ft,True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, num_classes).to(device)\n",
        "input_size = 224\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "561f70b3cf33455294cb6aebe4da1803",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/127M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.0468 Acc: 0.1429\n",
            "test Loss: 1.8197 Acc: 0.3333\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 1.8369 Acc: 0.2698\n",
            "test Loss: 1.5515 Acc: 0.4286\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 1.4682 Acc: 0.6190\n",
            "test Loss: 1.3637 Acc: 0.5714\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 1.2647 Acc: 0.7460\n",
            "test Loss: 1.1806 Acc: 0.7619\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 1.0504 Acc: 0.8254\n",
            "test Loss: 1.1075 Acc: 0.7143\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.9323 Acc: 0.8413\n",
            "test Loss: 1.0213 Acc: 0.6667\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.8097 Acc: 0.8889\n",
            "test Loss: 0.9819 Acc: 0.6667\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.7128 Acc: 0.9524\n",
            "test Loss: 0.9716 Acc: 0.7143\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.7038 Acc: 0.9524\n",
            "test Loss: 0.9606 Acc: 0.7143\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.6957 Acc: 0.9683\n",
            "test Loss: 0.9546 Acc: 0.7143\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.6882 Acc: 0.9683\n",
            "test Loss: 0.9507 Acc: 0.7143\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.6812 Acc: 0.9683\n",
            "test Loss: 0.9475 Acc: 0.7143\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.6752 Acc: 0.9683\n",
            "test Loss: 0.9445 Acc: 0.7143\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.6696 Acc: 0.9683\n",
            "test Loss: 0.9412 Acc: 0.7143\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.6627 Acc: 0.9683\n",
            "test Loss: 0.9408 Acc: 0.7143\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.6622 Acc: 0.9683\n",
            "test Loss: 0.9404 Acc: 0.7143\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.6616 Acc: 0.9683\n",
            "test Loss: 0.9400 Acc: 0.7143\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.6610 Acc: 0.9683\n",
            "test Loss: 0.9396 Acc: 0.7143\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.6605 Acc: 0.9683\n",
            "test Loss: 0.9391 Acc: 0.7143\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.6599 Acc: 0.9683\n",
            "test Loss: 0.9387 Acc: 0.7143\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.6593 Acc: 0.9683\n",
            "test Loss: 0.9383 Acc: 0.7143\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.6586 Acc: 0.9683\n",
            "test Loss: 0.9383 Acc: 0.7143\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.6585 Acc: 0.9683\n",
            "test Loss: 0.9383 Acc: 0.7143\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.6585 Acc: 0.9683\n",
            "test Loss: 0.9382 Acc: 0.7143\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.6584 Acc: 0.9683\n",
            "test Loss: 0.9382 Acc: 0.7143\n",
            "\n",
            "Training complete in 22m 2s\n",
            "Best val Acc: 0.761905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALojw_R5LrRt"
      },
      "source": [
        "## Problem 2: Video Classification\n",
        "### Previous Implementation\n",
        "This dataset was released as a part of the [paper](http://michaelryoo.com/papers/cvpr2013_ryoo.pdf) in CVPR 2013. The paper investigates multichannel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. As stated in the paper, *We first introduce video features designed to capture\n",
        "global motion (Subsection 3.1) and local motion (Subsection 3.2) observed during humans’ various interactions with\n",
        "the observer. Next, in Subsection 3.3, we cluster features to\n",
        "form visual words and obtain histogram representations. In\n",
        "Subsection 3.4, multi-channel kernels are described.* These features were prepared for an input to the SVM.\n",
        "\n",
        "###Using CNNs\n",
        "In this approach of video classification we are using an image classifier on every single frame of the video. We then have to merge the feature vectors obtained per frames using a fusion layer. This need to be built into the network itself. A Fusion layer is used to merge the output of separate networks that operate on temporally distant frames. It is normally implemented using the max pooling, average pooling or flattening technique. We then define a fully connected layer to provide the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywkjUNVop-Ku"
      },
      "source": [
        "### Problem 2.1\n",
        "### Temporal Pooling: 20 points\n",
        "As suggested in this [paper](https://arxiv.org/abs/1503.08909), we position the temporal pooling layer right before the ﬁrst fully connected layer as illustrated. This layer performs either mean-pooling or max-pooling across all video frames. The structure of the CNN-component is identical single-frame model. This network is able to collect all the spatial features in a given time window. However, the order of the temporal events is lost due to the nature of pooling across frames\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLGNfJ3Yeyfd"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Utilities for Temporal pooling\n",
        "#--------------------------------------------------\n",
        "\n",
        "def temporal_pooling(feature_vectors, pooling_shape):\n",
        "  ##########--WRITE YOUR CODE HERE--##########\n",
        "  # check the shape \n",
        "  maxpool = nn.MaxPool2d(pooling_shape, stride=(1, 1))\n",
        "  feature_vectors_pooled = maxpool(feature_vectors)\n",
        "\n",
        "  ##########-------END OF CODE-------##########\n",
        "  return feature_vectors_pooled\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5-k0xWu8WFw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrUagtl_hEcG"
      },
      "source": [
        "#above method is used after the feature extraction part below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlcLiUeUnVmM"
      },
      "source": [
        "### Problem 2.2\n",
        "### Network Definition: 20 points\n",
        "### Feature Extraction using an ImageNet pre-trained CNN \n",
        "Use a fine-tuned resNet model that you used in Part1 to extract the features from every video frames.\n",
        "\n",
        "Training new alexnet by training on 2d images for feature extraction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjFktNCBxnN_"
      },
      "source": [
        "def train_model_for_imgs(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                lim = min(batch_size,len(inputs))\n",
        "                inp = [inputs[i][j] for i in range(lim) for j in range(n_frames)]\n",
        "                lab = [labels[i] for i in range(lim) for j in range(n_frames)]\n",
        "                inputs = torch.stack(inp)\n",
        "                labels = torch.stack(lab)\n",
        "                # print(inputs.shape)\n",
        "                # print(labels.shape)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels-1\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # print(preds)\n",
        "                    # print(labels)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / data_siz[phase]\n",
        "            epoch_acc = running_corrects.double() / data_siz[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f74d922aa5ab45638164cdacde73ec0c",
            "1a3bb4e35c1b4a048cb4bf76ad430351",
            "2e02ed5a83514724984416732700ef11",
            "8c76e72cdd5843e4a613219b25469a0c",
            "b57b42aada7246eb891a7b487a5b22d6",
            "6e55537965fc45d19905164e38d47075",
            "3ffb5dd056ce46d8993be60c3e2876a4",
            "ce6ebce546334ed89a1069b13f9ce12d",
            "4564d782bd0f40138739e408029e2120",
            "8b3f261be53a4cba84e7a4a67aae316f",
            "3ccd276c75ea4f91a3d9d930c8bdb8ab"
          ]
        },
        "id": "QDWNweYIxt18",
        "outputId": "cb135a69-3d34-49f0-b332-351934cb8ae4"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#       Fine-Tune Pretrained Network\n",
        "#--------------------------------------------------\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "\n",
        "num_classes = 7\n",
        "\n",
        "def removeLayer(alexnet):    \n",
        "    temp = nn.Sequential(*list(alexnet.classifier.children())[:-1])\n",
        "    alexnet.classifier = temp\n",
        "    return alexnet\n",
        "\n",
        "model_ft_img = models.alexnet(pretrained=True)\n",
        "# for param in model_ft_img.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model_ft_img = removeLayer(model_ft_img)\n",
        "# print(model_ft_img)\n",
        "\n",
        "\n",
        "\n",
        "num_ftrs = model_ft_img.classifier[6].in_features\n",
        "layers = list(model_ft_img.classifier.children())[:-1]\n",
        "layers.extend([nn.Linear(num_ftrs,num_classes)])\n",
        "model_ft_img.classifier = nn.Sequential(*layers)\n",
        "model_ft_img.to(device)\n",
        "print(model_ft_img.classifier)\n",
        "# input_size = 224\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft_img.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "data_siz = {'train':len(train_ds)*n_frames,'test':len(test_ds)*n_frames}\n",
        "model_ft_img = train_model_for_imgs(model_ft_img, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=15)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f74d922aa5ab45638164cdacde73ec0c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Dropout(p=0.5, inplace=False)\n",
            "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Dropout(p=0.5, inplace=False)\n",
            "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            ")\n",
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 2.0488 Acc: 0.1151\n",
            "test Loss: 1.6601 Acc: 0.3958\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 1.5446 Acc: 0.4395\n",
            "test Loss: 1.3645 Acc: 0.4970\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 1.0445 Acc: 0.6290\n",
            "test Loss: 1.2395 Acc: 0.3899\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.7472 Acc: 0.7421\n",
            "test Loss: 1.2000 Acc: 0.4554\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.5327 Acc: 0.8204\n",
            "test Loss: 1.2453 Acc: 0.4613\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.3778 Acc: 0.8780\n",
            "test Loss: 1.2835 Acc: 0.5238\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.2851 Acc: 0.9067\n",
            "test Loss: 1.2981 Acc: 0.5744\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.2245 Acc: 0.9365\n",
            "test Loss: 1.2713 Acc: 0.5952\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.2015 Acc: 0.9444\n",
            "test Loss: 1.2475 Acc: 0.5952\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.1981 Acc: 0.9405\n",
            "test Loss: 1.2477 Acc: 0.5744\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.2001 Acc: 0.9435\n",
            "test Loss: 1.2422 Acc: 0.5863\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.1859 Acc: 0.9454\n",
            "test Loss: 1.2451 Acc: 0.5863\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.1777 Acc: 0.9593\n",
            "test Loss: 1.2515 Acc: 0.5982\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.1749 Acc: 0.9563\n",
            "test Loss: 1.2722 Acc: 0.6101\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.1630 Acc: 0.9643\n",
            "test Loss: 1.2730 Acc: 0.6071\n",
            "\n",
            "Training complete in 2m 26s\n",
            "Best val Acc: 0.610119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbc6mKuk7i0D"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(model_ft_img, open(\"/content/gdrive/MyDrive/Kamat_SaahilSuhas_114360951_PA4/model_trained_on_frames.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocg2JoUZLPv8"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/MyDrive/Kamat_SaahilSuhas_114360951_PA4/model_ft_img.pkl\", \"rb\") as f:\n",
        "  model_ft_img = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmFr2LC0LjtW"
      },
      "source": [
        "def removeLayer(alexnet):    \n",
        "    temp = nn.Sequential(*list(alexnet.classifier.children())[:-1])\n",
        "    alexnet.classifier = temp\n",
        "    return alexnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiM4dfX8fBE_",
        "outputId": "d0b1c685-ec7c-42a8-c423-8af4fea2ae8d"
      },
      "source": [
        "alexnet = removeLayer(model_ft_img)\n",
        "train_features = []\n",
        "test_features = []\n",
        "vidp = []\n",
        "for inputs, labels in dataloaders['train']:\n",
        "  out = []\n",
        "  for i in range(min(len(inputs),batch_size)):\n",
        "    out.append(alexnet(inputs[i]))\n",
        "  out = torch.stack(out)\n",
        "  train_features.append(temporal_pooling(out,(16,3)))\n",
        "\n",
        "for inputs, labels in dataloaders['test']:\n",
        "  out = []\n",
        "  for i in range(min(len(inputs),batch_size)):\n",
        "    out.append(alexnet(inputs[i]))\n",
        "  out = torch.stack(out)\n",
        "  test_features.append(temporal_pooling(out,(16,3)))\n",
        "\n",
        "# temporalp = \n",
        "# print(temporalp.shape)\n",
        "# train_features = torch.stack(train_features)\n",
        "# vidp = temporal_pooling(train_features,(3,3))\n",
        "\n",
        "# print(vidp.shape)\n",
        "train_features = torch.cat(train_features,dim=0)\n",
        "test_features = torch.cat(test_features,dim=0)\n",
        "\n",
        "train_features = torch.squeeze(train_features)\n",
        "test_features = torch.squeeze(test_features)\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([63, 4094])\n",
            "torch.Size([21, 4094])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzEaPw-kMudB"
      },
      "source": [
        "### Problem 2.2\n",
        "**Train and Test:10 points**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVuY2ZyRHaE"
      },
      "source": [
        "Training and testing SVC classifier on the features extracted using alexnet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4eSMNYKEgXJ",
        "outputId": "fbf79123-8e7f-497c-c0e8-e26ffb1f0c4d"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "#14 - 3.445\n",
        "#C=11.0115\n",
        "train_SVC = SVC(C=10,max_iter=5000)\n",
        "cpu = 'cpu'\n",
        "train_data = train_features.to(cpu)\n",
        "train_data = train_data.data.numpy()\n",
        "labels = np.array(train_labels)\n",
        "print(labels)\n",
        "train_SVC.fit(train_data,labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 5 2 5 7 4 4 3 7 5 3 2 6 6 2 1 3 3 7 1 6 4 5 7 1 3 5 1 3 4 2 1 1 7 3 5 4\n",
            " 1 2 4 6 2 4 6 7 2 5 7 1 2 3 5 7 6 6 6 4 6 4 1 3 5 2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, max_iter=5000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYGTkfC5brT2",
        "outputId": "d5df58ff-6b29-4fa2-ebc3-49154d00dc55"
      },
      "source": [
        "test_data = test_features.to(cpu)\n",
        "test_data = test_data.data.numpy()\n",
        "test_lab = np.array(test_labels)\n",
        "test_label_pred_SVC = train_SVC.predict(test_data)\n",
        "  \n",
        "#Evaluation\n",
        "\n",
        "#The prediction is test_label_pred\n",
        "print(\"Predicted:\\t\\t\" + str(test_label_pred_SVC))\n",
        "print(\"Ground truth labels:\\t\" + str(test_lab))\n",
        "accuracy = sum(np.array(test_label_pred_SVC) == test_lab) / float(len(test_lab))\n",
        "print(\"The accuracy of SVM model on temporal pooled features is {:.2f}%\".format(accuracy*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted:\t\t[5 1 4 6 7 6 2 5 3 5 6 4 1 2 5 2 5 6 4 1 6]\n",
            "Ground truth labels:\t[5 1 4 3 7 6 2 5 3 7 6 4 1 3 5 2 7 6 4 1 2]\n",
            "The accuracy of SVM model on temporal pooled features is 76.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt0AxBqH6iTS"
      },
      "source": [
        "### Problem 2.2\n",
        "**Fusion based implementation: 20 bonus points**\n",
        "\n",
        "--->Implemented late pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLG3WtEmYYM3"
      },
      "source": [
        "#--------------------------------------------------\n",
        "#Define your  Vid_Classifier\n",
        "#you may add extra parameters here.\n",
        "#remember to define :\n",
        "  # a base model which is an ImageNet pre-trained CNN : Extract One feature vector per frame\n",
        "  # a max pooling layer that finds the maximum feature map over a local temporal neighborhood\n",
        "  # a fully connected layer to unify the feature maps\n",
        "#You may also want to include other parameters for your module\n",
        "#if you are using a knn classifer, please indicate it well with your code.\n",
        "#--------------------------------------------------\n",
        "  \n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "class Vid_Classifier(nn.Module):\n",
        "    def __init__(self, params_model):\n",
        "        super(Vid_Classifier, self).__init__()\n",
        "        num_classes = params_model[\"num_classes\"]\n",
        "        dr_rate= params_model[\"dr_rate\"] #drop out rate\n",
        "        pretrained = params_model[\"pretrained\"]\n",
        "        #--------------------------------------------------\n",
        "        #Your code here\n",
        "        #late pooling\n",
        "        resnet = models.resnet18(pretrained)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.resnet = resnet\n",
        "        self.fc1 = nn.Linear(1000, 800)\n",
        "        self.fc2 = nn.Linear(800,612)\n",
        "        \n",
        "        \n",
        "        self.fc3 = nn.Linear(610,num_classes)\n",
        "        #--------------------------------------------------\n",
        "             \n",
        "    def forward(self, x):\n",
        "        #--------------------------------------------------\n",
        "        #Your code here\n",
        "        #--------------------------------------------------\n",
        "        b_z, ts, c, h, w = x.shape\n",
        "\n",
        "        out = []\n",
        "        #late pooling - passing frames for each video through resnet and 2 fully connected layers\n",
        "        for i in range(len(x)):\n",
        "          im = self.resnet(x[i])\n",
        "          im = self.fc1(im)\n",
        "          im = self.fc2(im)\n",
        "          out.append(im)\n",
        "        out = torch.stack(out)\n",
        "        out = temporal_pooling(out,(16,3))\n",
        "        out = torch.squeeze(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1r7_tWN4jLb"
      },
      "source": [
        "num_classes = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btOal_ampEnm"
      },
      "source": [
        "params_model={\n",
        "        \"num_classes\": num_classes,\n",
        "        \"dr_rate\": 0.1,\n",
        "        \"pretrained\" : True,}\n",
        "model = Vid_Classifier(params_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i69EIC7B5EUc"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgdQLe0VWmaD"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7g73PBgKA-e",
        "outputId": "3171321e-de83-4190-b4d7-89d2a87f9202"
      },
      "source": [
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(dataloaders['train']):\n",
        "        labels-=1\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backprop and perform optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "\n",
        "        if(i+1)%8==0:\n",
        "          print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch + 1, num_epochs, loss.item(),\n",
        "                          (correct / total) * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 2.0151, Accuracy: 14.29%\n",
            "Epoch [2/25], Loss: 1.7441, Accuracy: 42.86%\n",
            "Epoch [3/25], Loss: 1.5363, Accuracy: 85.71%\n",
            "Epoch [4/25], Loss: 1.3783, Accuracy: 100.00%\n",
            "Epoch [5/25], Loss: 1.2065, Accuracy: 100.00%\n",
            "Epoch [6/25], Loss: 1.0508, Accuracy: 100.00%\n",
            "Epoch [7/25], Loss: 0.9330, Accuracy: 100.00%\n",
            "Epoch [8/25], Loss: 0.8266, Accuracy: 100.00%\n",
            "Epoch [9/25], Loss: 0.7370, Accuracy: 100.00%\n",
            "Epoch [10/25], Loss: 0.6620, Accuracy: 100.00%\n",
            "Epoch [11/25], Loss: 0.5955, Accuracy: 100.00%\n",
            "Epoch [12/25], Loss: 0.5407, Accuracy: 100.00%\n",
            "Epoch [13/25], Loss: 0.4909, Accuracy: 100.00%\n",
            "Epoch [14/25], Loss: 0.4449, Accuracy: 100.00%\n",
            "Epoch [15/25], Loss: 0.4077, Accuracy: 100.00%\n",
            "Epoch [16/25], Loss: 0.3743, Accuracy: 100.00%\n",
            "Epoch [17/25], Loss: 0.3422, Accuracy: 100.00%\n",
            "Epoch [18/25], Loss: 0.3150, Accuracy: 100.00%\n",
            "Epoch [19/25], Loss: 0.2903, Accuracy: 100.00%\n",
            "Epoch [20/25], Loss: 0.2665, Accuracy: 100.00%\n",
            "Epoch [21/25], Loss: 0.2475, Accuracy: 100.00%\n",
            "Epoch [22/25], Loss: 0.2278, Accuracy: 100.00%\n",
            "Epoch [23/25], Loss: 0.2114, Accuracy: 100.00%\n",
            "Epoch [24/25], Loss: 0.1958, Accuracy: 100.00%\n",
            "Epoch [25/25], Loss: 0.1811, Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14qsKkQ-Tfun",
        "outputId": "8c7876eb-dd35-4232-d456-bf54aaccba52"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in dataloaders['test']:\n",
        "    labels-=1\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print('Accuracy on test set: {:.2f}%'.format((correct/total)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 42.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRw9JWtMSrWi"
      },
      "source": [
        "**Answer**:\n",
        "\n",
        "Accuracy on test set: 42.86%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrscW69XWmSt"
      },
      "source": [
        "path2weights = \"/content/gdrive/MyDrive/Kamat_SaahilSuhas_114360951_PA4/fusion_cnn_weights.pt\"\n",
        "torch.save(model.state_dict(), path2weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Ng2cjpXhT2"
      },
      "source": [
        "#define your training function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGefIYpYYNF"
      },
      "source": [
        "## Submission guidelines\n",
        "---\n",
        "Your need to submit a single zip file to Blackboard described as follow.\n",
        "\n",
        "Please generate a pdf file that includes a ***google shared link*** (explained in the next paragraph). This pdf file should be named as ***Surname_Givenname_SBUID_pa*\\*.pdf** (example: Jordan_Michael_111234567_pa3.pdf for this assignment).\n",
        "\n",
        "To generate the ***google shared link***, first create a folder named ***Surname_Givenname_SBUID_pa**** in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. For instance in this homework:\n",
        "\n",
        "```\n",
        "Surname_Givenname_SBUID_pa4\n",
        "        |---data\n",
        "        |---CSE527-PA4-fall21.ipynb\n",
        "```\n",
        "Note that this folder should be in your Google Drive with your Stony Brook account.\n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter the TA's email: ***bjha@cs.stonybrook.edu***, ***li.wenchen@stonybrook.edu***, ***yifeng.huang@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **UNCHECK** the **Notify people** box.\n",
        "\n",
        "Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in blackboard. \n",
        "\n",
        "To submit to Blackboard, zip ***Surname_Givenname_SBUID_pa*\\*.pdf** and ***Surname_Givenname_SBUID_pa**** folder together and name your zip file as ***Surname_Givenname_SBUID_pa*\\*.zip**. \n",
        "\n",
        "**DO NOT upload the datasets to Blackboard.**\n",
        "\n",
        "The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_pa4' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n",
        "\n",
        "\n",
        "-- DO NOT change the folder structure, please just fill in the blanks. <br>\n",
        "\n",
        "You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n",
        "\n",
        "If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJOi8QYYYNG"
      },
      "source": [
        "<!--Write your report here in markdown or html-->\n"
      ]
    }
  ]
}